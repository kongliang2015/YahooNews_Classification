{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora,models,similarities,utils\n",
    "import logging\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from keras.preprocessing import text,sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取训练数据\n",
    "def getTrainSet(inFile):\n",
    "    # 训练集\n",
    "    train_set=[]\n",
    "    # 情感标签集\n",
    "    target_set = []\n",
    "    # 统计所有出现的词\n",
    "    word_ctr = collections.Counter()\n",
    "    # 评论的最大长度\n",
    "    maxlen = 0\n",
    "    len_ctr = collections.Counter()\n",
    "    \n",
    "    # 读入训练数据           \n",
    "    f=open(inFile)\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        article = line.replace('\\n','').split(\" \")\n",
    "\n",
    "        # 情感标签\n",
    "        target_set.append(getLable(article[0]))\n",
    "\n",
    "        # 内容\n",
    "        content = article[1:]\n",
    "        train_set.append(content)\n",
    "\n",
    "        # 获得评论的最大长度\n",
    "        if len(content) > maxlen:\n",
    "            maxlen = len(content)\n",
    "\n",
    "        # 统计各种长度的文章个数\n",
    "        len_ctr[str(len(content))] += 1\n",
    "\n",
    "        # 统计所有出现的词\n",
    "        for w in content:\n",
    "            word_ctr[w] += 1\n",
    "\n",
    "    f.close()\n",
    "        \n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 如果输入分布在硬盘上的多个文件中, 文件的每一行是一个句子, 那么可以逐个文件, 逐行的处理输入\n",
    "# # 这样处理可以不必一次性把所有句子都载入内存\n",
    "# class MySentences(object):\n",
    "#     def __init__(self, dirname):\n",
    "#         self.dirname = dirname\n",
    "#     def __iter__(self):\n",
    "#         for fname in os.listdir(self.dirname):\n",
    "#             for line in open(os.path.join(self.dirname, fname)):\n",
    "#                 yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练word2vec\n",
    "def trainModel(inFile,modelFile,vecFile):\n",
    "    # 读入数据    \n",
    "    data_set = getTrainSet(inFile)\n",
    "    \n",
    "    # 训练\n",
    "    # 少于min_count次数的单词会被丢弃掉, 默认值为5\n",
    "    # size = 神经网络的隐藏层的单元数 default value is 100\n",
    "    # workers= 控制训练的并行:default = 1 worker (no parallelization) 只有在安装了Cython后才有效\n",
    "    model = models.Word2Vec(data_set,min_count=5,window=10,size = 128,workers=2)\n",
    "    \n",
    "    # 存储模型\n",
    "    model.save(modelFile)\n",
    "    \n",
    "    # 存储vector\n",
    "    model.wv.save_word2vec_format(vecFile, binary=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把原始文本转化为由词汇表索引表示的矩阵\n",
    "def fastBuildSeq(inFile,outFile,modelFile,vecFile):\n",
    "    # 读入数据\n",
    "    title_set,data_set = getTrainSet(inFile)\n",
    "    \n",
    "    # 装载模型\n",
    "    model = models.Word2Vec.load(modelFile)\n",
    "    word_vec = model.wv.load_word2vec_format(vecFile, binary=True) \n",
    "    \n",
    "    # 使用dir(object)查看对象的属性\n",
    "    # 对每一个文章做转换      \n",
    "    # 注意：由于word2vec的向量在训练的时候用的是unicode的编码，\n",
    "    # 所以在字典里面匹配key的时候，需要把key转化为unicode的编码，使用decode('utf-8')\n",
    "    transfrom = []\n",
    "    for news in data_set:\n",
    "        trs_news = [word_vec.vocab[w.decode('utf-8')].index for w in news if w.decode('utf-8') in word_vec.vocab]\n",
    "#         # --- 调试\n",
    "#         trs_news = []\n",
    "#         for w in news:\n",
    "#             if w.decode('utf-8') in word_vec.vocab:\n",
    "#                 print \"in vocab = \",w.decode('utf-8')\n",
    "#                 trs_news.append((word_vec.vocab[w.decode('utf-8')].index,w))\n",
    "#         # --\n",
    "        transfrom.append(trs_news)\n",
    "    \n",
    "#     for x in transfrom:\n",
    "#         print x\n",
    "    \n",
    "    # 对文字序列做补齐 ，补齐长度=最长的文章长度 ，补齐在最后，补齐用的词汇默认是词汇表index=0的词汇，也可通过value指定\n",
    "    # 训练好的w2v词表的index = 0 对应的词汇是空格\n",
    "    result_data = sequence.pad_sequences(transfrom,maxlen=200,padding='post')\n",
    "    \n",
    "#     print result_data\n",
    "    \n",
    "#     for x in result_data:\n",
    "#         print x\n",
    "    \n",
    "    np.save(outFile,result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "   \n",
    "    # 定义文件路径\n",
    "    dataPath = \"/home/hadoop/DataSencise/bdci2017/BDCI2017-360/data/\"\n",
    "    mdlPath = \"/home/hadoop/DataSencise/bdci2017/BDCI2017-360/model/\"\n",
    "    \n",
    "    # 定义训练文件名\n",
    "    inFile = dataPath + \"train/train_output_title.tsv\"\n",
    "    # 定义输出文件名\n",
    "    modelFile = mdlPath + \"w2v_train.mdl\"\n",
    "    vecFile = mdlPath + \"w2v_v1.bin\"\n",
    "   \n",
    "    \n",
    "    # 训练模型\n",
    "    trainModel(inFile,modelFile,vecFile)\n",
    "    \n",
    "    # 测试模型\n",
    "#     testModel(modelFile,vecFile)\n",
    "    \n",
    "    # 把分词以后的文本转化为供CNN训练的数据文件\n",
    "#     fastBuildSeq(inFile,outFile,modelFile,vecFile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
